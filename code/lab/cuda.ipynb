{
 "metadata": {
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.13 64-bit ('pytorch': conda)"
  },
  "interpreter": {
   "hash": "628a95ef4e044bb3a862cae43fd368045431338c356870eba45002d3214ff2f7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "gpu 是否可用 True\ngpu 数量 1\n当前 gpu index 0\ngpu 名称 Tesla P4\ntensor([[0.0067, 0.8844, 0.4384],\n        [0.0243, 0.1374, 0.3897]])\ntensor([[0.0067, 0.8844, 0.4384],\n        [0.0243, 0.1374, 0.3897]], device='cuda:0')\nSequential(\n  (0): Linear(in_features=12, out_features=24, bias=True)\n  (1): ReLU()\n)\n"
     ]
    }
   ],
   "source": [
    "print('gpu 是否可用',torch.cuda.is_available())\n",
    "\n",
    "print('gpu 数量', torch.cuda.device_count())\n",
    "\n",
    "print('当前 gpu index', torch.cuda.current_device())\n",
    "\n",
    "print('gpu 名称', torch.cuda.get_device_name(0))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "a = torch.rand(2,3)\n",
    "print(a)\n",
    "a = a.cuda(0)\n",
    "print(a)\n",
    "\n",
    "model = torch.nn.Sequential(torch.nn.Linear(12, 24), torch.nn.ReLU()).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}