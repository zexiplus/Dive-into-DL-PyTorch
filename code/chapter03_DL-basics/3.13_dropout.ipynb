{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.8.1\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"..\") \n",
    "import d2lzh_pytorch as d2l\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dropout(X, drop_prob):\n",
    "    X = X.float()\n",
    "    assert 0 <= drop_prob <= 1\n",
    "    keep_prob = 1 - drop_prob\n",
    "    # 这种情况下把全部元素都丢弃\n",
    "    if keep_prob == 0:\n",
    "        return torch.zeros_like(X)\n",
    "    # torch.rand returns a tensor filled with random numbers from a uniform distribution on the interval [0, 1)\n",
    "    mask = (torch.rand(X.shape) < keep_prob).float()\n",
    "    \n",
    "    # 某些元素归 0, 某些扩大\n",
    "    return mask * X / keep_prob \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "origin X\n tensor(120)\ndropout 0\n tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11., 12., 13., 14., 15.]]) tensor(120.)\ndropout 0.5\n tensor([[ 0.,  2.,  4.,  0.,  0., 10.,  0.,  0.],\n        [16.,  0.,  0., 22.,  0.,  0., 28.,  0.]]) tensor(82.)\n"
     ]
    }
   ],
   "source": [
    "X = torch.arange(16).view(2, 8)\n",
    "X_drop_0 = dropout(X, 0)\n",
    "X_drop_05 = dropout(X, 0.5)\n",
    "print('origin X\\n', X.sum())\n",
    "\n",
    "print('dropout 0\\n', X_drop_0, X_drop_0.sum())\n",
    "\n",
    "print('dropout 0.5\\n', X_drop_05, X_drop_05.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "len(mnist_train) 60000\nlen(mnist_test) 10000\ntensor([[-0.0074,  0.0095, -0.0035,  ...,  0.0124,  0.0058, -0.0042],\n        [ 0.0073, -0.0075, -0.0042,  ...,  0.0023,  0.0010,  0.0002],\n        [ 0.0086, -0.0140, -0.0075,  ..., -0.0030,  0.0179, -0.0003],\n        ...,\n        [ 0.0071,  0.0027,  0.0003,  ..., -0.0006, -0.0082, -0.0028],\n        [ 0.0083,  0.0016, -0.0102,  ..., -0.0283, -0.0167, -0.0107],\n        [-0.0171,  0.0008,  0.0097,  ...,  0.0016,  0.0099,  0.0042]],\n       requires_grad=True)\ntensor([[-0.0074,  0.0095, -0.0035,  ...,  0.0124,  0.0058, -0.0042],\n        [ 0.0073, -0.0075, -0.0042,  ...,  0.0023,  0.0010,  0.0002],\n        [ 0.0086, -0.0140, -0.0075,  ..., -0.0030,  0.0179, -0.0003],\n        ...,\n        [ 0.0071,  0.0027,  0.0003,  ..., -0.0006, -0.0082, -0.0028],\n        [ 0.0083,  0.0016, -0.0102,  ..., -0.0283, -0.0167, -0.0107],\n        [-0.0171,  0.0008,  0.0097,  ...,  0.0016,  0.0099,  0.0042]],\n       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256\n",
    "drop_prob1, drop_prob2 = 0.2, 0.5\n",
    "num_epochs, lr, batch_size = 10, 100.0, 256 # 这里的学习率设置的很大，原因同 3.9.6 节。\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "\n",
    "W1 = torch.tensor(np.random.normal(0, 0.01, size=(num_inputs, num_hiddens1)), dtype=torch.float, requires_grad=True)\n",
    "b1 = torch.zeros(num_hiddens1, requires_grad=True)\n",
    "W2 = torch.tensor(np.random.normal(0, 0.01, size=(num_hiddens1, num_hiddens2)), dtype=torch.float, requires_grad=True)\n",
    "b2 = torch.zeros(num_hiddens2, requires_grad=True)\n",
    "W3 = torch.tensor(np.random.normal(0, 0.01, size=(num_hiddens2, num_outputs)), dtype=torch.float, requires_grad=True)\n",
    "b3 = torch.zeros(num_outputs, requires_grad=True)\n",
    "\n",
    "W1_ = W1.clone().detach().requires_grad_(True)\n",
    "b1_ = b1.clone().detach().requires_grad_(True)\n",
    "W2_ = W2.clone().detach().requires_grad_(True)\n",
    "b2_ = b2.clone().detach().requires_grad_(True)\n",
    "W3_ = W3.clone().detach().requires_grad_(True)\n",
    "b3_ = b3.clone().detach().requires_grad_(True)\n",
    "\n",
    "print(W1)\n",
    "print(W1_)\n",
    "\n",
    "params = [W1, b1, W2, b2, W3, b3]\n",
    "params2 = [W1_, b1_, W2_, b2_, W3_, b3_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def net(X, is_training=True):\n",
    "    X = X.view(-1, num_inputs) # 将输入变形为形状 (batch_size, num_inputs)\n",
    "    H1 = (torch.matmul(X, W1) + b1).relu()\n",
    "    if is_training:  # 只在训练模型时使用丢弃法\n",
    "        H1 = dropout(H1, drop_prob1)  # 在第一层全连接后添加丢弃层\n",
    "    H2 = (torch.matmul(H1, W2) + b2).relu()\n",
    "    if is_training:\n",
    "        H2 = dropout(H2, drop_prob2)  # 在第二层全连接后添加丢弃层\n",
    "    return torch.matmul(H2, W3) + b3\n",
    "\n",
    "def net_without_dropout(X, is_training=True):\n",
    "    X = X.view(-1, num_inputs) # 将输入变形为形状 (batch_size, num_inputs)\n",
    "    H1 = (torch.matmul(X, W1_) + b1_).relu()\n",
    "    H2 = (torch.matmul(H1, W2_) + b2_).relu()\n",
    "    return torch.matmul(H2, W3_) + b3_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 1, loss 0.0045, train acc 0.555, test acc 0.738\n",
      "epoch 2, loss 0.0023, train acc 0.786, test acc 0.746\n",
      "epoch 3, loss 0.0019, train acc 0.821, test acc 0.810\n",
      "epoch 4, loss 0.0017, train acc 0.840, test acc 0.840\n",
      "epoch 5, loss 0.0016, train acc 0.850, test acc 0.831\n",
      "epoch 6, loss 0.0015, train acc 0.856, test acc 0.856\n",
      "epoch 7, loss 0.0015, train acc 0.862, test acc 0.860\n",
      "epoch 8, loss 0.0014, train acc 0.866, test acc 0.848\n",
      "epoch 9, loss 0.0014, train acc 0.871, test acc 0.833\n",
      "epoch 10, loss 0.0014, train acc 0.873, test acc 0.857\n",
      "epoch 1, loss 0.0044, train acc 0.572, test acc 0.730\n",
      "epoch 2, loss 0.0023, train acc 0.786, test acc 0.753\n",
      "epoch 3, loss 0.0018, train acc 0.829, test acc 0.832\n",
      "epoch 4, loss 0.0016, train acc 0.848, test acc 0.837\n",
      "epoch 5, loss 0.0015, train acc 0.856, test acc 0.827\n",
      "epoch 6, loss 0.0014, train acc 0.867, test acc 0.852\n",
      "epoch 7, loss 0.0014, train acc 0.872, test acc 0.858\n",
      "epoch 8, loss 0.0013, train acc 0.877, test acc 0.852\n",
      "epoch 9, loss 0.0012, train acc 0.881, test acc 0.865\n",
      "epoch 10, loss 0.0012, train acc 0.886, test acc 0.868\n"
     ]
    }
   ],
   "source": [
    "# 对比带 dropout 与 不带 dropout 的预测准确率\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, params, lr)\n",
    "d2l.train_ch3(net_without_dropout, train_iter, test_iter, loss, num_epochs, batch_size, params2, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pytorch 实现\n",
    "net2 = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(num_inputs, num_hiddens1),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(drop_prob1),\n",
    "        nn.Linear(num_hiddens1, num_hiddens2), \n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(drop_prob2),\n",
    "        nn.Linear(num_hiddens2, 10)\n",
    ")\n",
    "\n",
    "net2_without_dropout = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(num_inputs, num_hiddens1),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(num_hiddens1, num_hiddens2), \n",
    "        nn.ReLU(),\n",
    "        nn.Linear(num_hiddens2, 10)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 1, loss 0.0035, train acc 0.669, test acc 0.758\n",
      "epoch 2, loss 0.0021, train acc 0.805, test acc 0.806\n",
      "epoch 3, loss 0.0018, train acc 0.831, test acc 0.769\n",
      "epoch 4, loss 0.0017, train acc 0.847, test acc 0.838\n",
      "epoch 5, loss 0.0016, train acc 0.853, test acc 0.844\n",
      "epoch 6, loss 0.0015, train acc 0.861, test acc 0.855\n",
      "epoch 7, loss 0.0015, train acc 0.864, test acc 0.842\n",
      "epoch 8, loss 0.0014, train acc 0.869, test acc 0.830\n",
      "epoch 9, loss 0.0014, train acc 0.873, test acc 0.859\n",
      "epoch 10, loss 0.0013, train acc 0.877, test acc 0.866\n",
      "epoch 1, loss 0.0034, train acc 0.671, test acc 0.798\n",
      "epoch 2, loss 0.0019, train acc 0.817, test acc 0.794\n",
      "epoch 3, loss 0.0017, train acc 0.842, test acc 0.785\n",
      "epoch 4, loss 0.0015, train acc 0.855, test acc 0.824\n",
      "epoch 5, loss 0.0014, train acc 0.864, test acc 0.853\n",
      "epoch 6, loss 0.0014, train acc 0.871, test acc 0.847\n",
      "epoch 7, loss 0.0013, train acc 0.877, test acc 0.859\n",
      "epoch 8, loss 0.0012, train acc 0.883, test acc 0.863\n",
      "epoch 9, loss 0.0012, train acc 0.885, test acc 0.863\n",
      "epoch 10, loss 0.0011, train acc 0.891, test acc 0.861\n"
     ]
    }
   ],
   "source": [
    "optimizer2 = torch.optim.SGD(net2.parameters(), lr=0.5)\n",
    "optimizer2_without_dropout = torch.optim.SGD(net2_without_dropout.parameters(), lr=0.5)\n",
    "\n",
    "d2l.train_ch3(net2, train_iter, test_iter, loss, num_epochs, batch_size, None, None, optimizer2)\n",
    "d2l.train_ch3(net2_without_dropout, train_iter, test_iter, loss, num_epochs, batch_size, None, None, optimizer2_without_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3613jvsc74a57bd06a7cafeaeb3fd68df71f631e022add281d537fd7827e106ecab671299a698970",
   "display_name": "Python 3.6.13 64-bit ('pytorch': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}